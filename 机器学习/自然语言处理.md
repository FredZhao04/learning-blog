# 自然语言处理
## 简介
自然语言处理是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言，特别是如何编程计算机以成功处理大量的自然语言数据。

NLP的基本任务包括正则表达式、分词、词法分析、语音识别、文本分类、信息检索、问答系统——如对一些问题进行回答或与用户进行交互——机器翻译等。常用的模型则有马科夫模型、朴素贝叶斯、循环神经网络等。

我们把处理口语和书面语（统称为”语言“）的计算机技术称为语音和语言处理，简称自然语言处理，这是一个范围很广泛的定义，这个定义包括了从人人皆知的诸如词数计算、自动换行等简单技术，直到诸如在web上的自动问答、实时的口语自动翻译等高级技术。

自然语言处理的这些应用与其他应用系统的区别是，自然语言处理要使用语言知识。例如，UNIX的wc程序可以用来计算文本文件中的字节数、词数或行数。当我们用它来计算字节数和行数时，wc只用于进行一般的数据处理。但是，当我们用它来计算一个文件中词的数目时，就需要关于”什么事一个词“的语言知识，这样，这个wc也就成为了一个自然语言处理系统。
## 发展历史
自然语言处理（NLP）大体是从上世纪50年代开始，1950年，图灵发表论文“Computing Machinery and Intelligence”，提出著名的“图灵测试”作为判断智能的条件。1948年Shannon把离散马尔科夫过程的概率模型应用于描述语言的自动机。Chomsky吸取了他的思想，首先把有限状态自动机作为一种工具来刻画语言的语法，并且把有限状态语言定义为由有限状态语法生成的语言。这些早起的研究工作产生了形式语言理论（formal language theory）这样的研究领域，采用代数和集合论把形式语言定义为符号的序列。

这一时期的研究都相当基础，1954年的Georgetown的实验试图将超过60句俄文全部自动翻译成为英文，其研究人员声称三到五年之内即可解决机器翻译的问题。不过这项工作的实际进展远低于预期，1966年的ALPAC报告发现研究未达预期目标，机器翻译的研究经费遭到大幅削减。一直到1980年代末期，统计机器翻译系统被发展出来，机器翻译的研究才得以更上一层楼。

1960年代发展特别成功的NLP系统包括Winograd提出的SHRDLU——一个词汇设限、运作于受限如“积木世界”的一种自然语言系统，以及1964-1966年Joseph Weizenbaum模拟“个人中心治疗”而设计的ELIZA——几乎未运用人类思想和感情的讯息，有时候却能呈现令人讶异地类似人之间的互动。但当“病人”提出的问题超出ELIZA极小的知识范围之时，可能会得到空泛的回答。例如问题是“我的头痛”，回答是“为什么说你头痛？”

这一时期研究者大多着重研究推理和逻辑问题，这些简单的系统把模式匹配和关键词搜索与简单试探的方法结合起来，进行推理和自动问答，它们都只能在某一个领域内使用。但也有一些统计学者和电子学的专业研究人员试图使用统计算法来解决这些问题。比如Bledsoe和Browning于1959年建立了用于文本识别的贝叶斯系统来计算字母系列的似然度，Mosteller和Wallace于1964年用贝叶斯方法来解决在The Federalist文章中的原作者的分布问题。这一期间还出现了第一个联机语料库：Brown美国英语语料库。

一直到1980年代，多数自然语言处理系统是以一套复杂、人工订定的规则为基础的，颇有专家系统（expert system）的味道。不过从1980年代末期开始，语言处理引进了机器学习的算法，NLP产生革新。其成因主要有两个：运算能力稳定增加（参见摩尔定律）；以及Chomskyan语言学理论渐渐丧失主导（例如转换-生成文法-transformational grammar）。该理论的架构不倾向于语料库——机器学习处理语言所用方法的基础。有些最早期使用的机器学习算法，例如决策树，是硬性的、“if-then”规则组成的系统，类似当时既有的人工订定的规则。不过词性标记将隐马尔可夫模型(HMM)引入NLP，并且研究日益聚焦于软性的、以概率做决定的统计模型，其基础是将输入资料里每一个特性赋予代表其分量的数值。这种模型通常足以处理非预期的输入数据，尤其是输入有错误时，并且在整合到包含多个子任务的较大系统时，结果比较可靠。

近来的研究更加聚焦于非监督式学习和半监督学习的算法。这种算法，能够从没有人工注解理想答案的资料里学习。大体而言，这种学习比监督学习困难，并且在同量的数据下，通常产生的结果较不准确。不过没有注解的数据量极巨，弥补了较不准确的缺点。

近年来, 随着深度学习的快速发展，用于自然语言处理的模型也大幅演化，2013年Tomas Mikolov及其团队提出了word2vec，为一群用来产生词向量的相关模型，在业界产生了巨大的影响。2016年Jozefowicz等学者在当前模型在语料库、词汇量以及复杂的长期语言结构方面进行了扩展，他们在十亿字基准上对诸如字符卷积神经网络（ character Convolutional Neural Networks ）或长期短期记忆（ Long-Short Term Memory）等技术进行了详尽的研究。
## 发展分析
### 瓶颈
目前分词的技术仍然是一个难点，特别是对于汉语这样缺乏明显词汇边界的语言，如何正确划分不同词语十分重要；

词义消岐是NLP的另一个难点，由于在某些情况下语境复杂，会给模型的训练带来困难；

另外就是由于语言的模糊性，缺乏能够解释语言形成的原理，也给我们模拟语言造成困难；

最后，能够通用于多种语言的模型的数量仍然十分不足。
### 未来发展方向
目前NLP领域仍需要大量研究，十分有潜力的方向有：

1. 独立于任务的NLP数据增强
2. 用于NLP的Few-shot learning
3. 用于NLP的迁移学习
4. 多任务学习
5. 跨语言学习
6. 独立于任务的架构提升

## Bag of Words Meets Bags of Popcorn
https://www.kaggle.com/c/word2vec-nlp-tutorial

IMDB的情感分析数据，也是NLP任务里一个经典的入门数据集。不管是传统文本分类方法，还是基于Embedding or DL的方法比赛中都给出了介绍和任务，有丰富的kernel，适合入门。如果希望通过这个项目收货更多建议多去对比不同模型得到的case，总结各类模型的特点。

## Sentiment Analysis on Movie Reviews
https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews

类似IMDB，这也是学术圈用的比较多的SA数据，除了IMDB类似的地方，可以通过这个项目了解一下stanford nlp、semantic parser、Recursive，都属于NLP常用的工具 and 理论。
## Text Normalization Challenge - English Language
https://www.kaggle.com/c/text-normalization-challenge-english-language

一个非常基础的NLP的data preprocessing，通过真正做一下，可以了解NLP面临的一些底层问题，以及做高级任务的时候面临的数据并非完全正确的raw data。这个有姐妹篇比赛，Russian Text Normalization Challenge。其实很多事情是想通的，多做一些对于视野、未来的其他任务也是有帮助的。

## Crowdflower Search Results Relevance
电商搜索结果相关性的比赛，可以说也是非常实际的需求了。有大量开放的solution，包括第一名Chenglong Chen的分享，当年受益匪浅，进他kaggle主页可以看到一些类似or相关的NLP比赛，都是做完就可以实际上手真实任务的那种。maybe SNLI的收货也可以在这个任务上试试，当年的方法基本都是特征工程。
## Toxic Comment Classification Challenge 
比较近期的一个比赛了，又是文本分类，但是多分类 and 场景更加实际。基本都是DL based，作为学习目的可以多去看一下数据，相对来讲这个数据更符合真实场景。SA的学术数据相对来讲更加理想化（干净）。

## 词形还原（Lemmatization）和词干提取（stemming）
词形还原是文本预处理中的重要部分，与词干提取（stemming）很相似。

简单说来，词形还原就是去掉单词的词缀，提取单词的主干部分，通常提取后的单词会是字典中的单词，不同于词干提取（stemming），提取后的单词不一定会出现在单词中。比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。

词形还原（lemmatization），是把一个任何形式的语言词汇还原为一般形式（能表达完整语义），而词干提取（stemming）是抽取词的词干或词根形式（不一定能够表达完整语义）。词形还原和词干提取是词形规范化的两类
重要方式，都能够达到有效归并词形的目的，二者既有联系也有区别。

目标一致。词干提取和词形还原的目标均为将词的屈折形态或派生形态简化或归并为词干（stem）或原形的基础形式，都是一种对词的不同形态的统一归并的过程。
结果部分交叉。词干提取和词形还原不是互斥关系，其结果是有部分交叉的。一部分词利用这两类方法都能达到相同的词形转换效果。如“dogs”的词干为“dog”，其原形也为“dog”。

主流实现方法类似。目前实现词干提取和词形还原的主流实现方法均是利用语言中存在的规则或利用词典映射提取词干或获得词的原形。
应用领域相似。主要应用于信息检索和文本、自然语言处理等方面，二者均是这些应用的基本步骤。

### 区别

在原理上，词干提取主要是采用“缩减”的方法，将词转换为词干，如将“cats”处理为“cat”，将“effective”处理为“effect”。而词形还原主要采用“转变”的方法，将词转变为其原形，如将“drove”处理为“drive”，将“driving”处理为“drive”。

在复杂性上，词干提取方法相对简单，词形还原则需要返回词的原形，需要对词形进行分析，不仅要进行词缀的转化，还要进行词性识别，区分相同词形但原形不同的词的差别。词性标注的准确率也直接影响词形还原的准确率，因此，词形还原更为复杂。

在实现方法上，虽然词干提取和词形还原实现的主流方法类似，但二者在具体实现上各有侧重。词干提取的实现方法主要利用规则变化进行词缀的去除和缩减，从而达到词的简化效果。词形还原则相对较复杂，有复杂的形态变化，单纯依据规则无法很好地完成。其更依赖于词典，进行词形变化和原形的映射，生成词典中的有效词。
在结果上，词干提取和词形还原也有部分区别。词干提取的结果可能并不是完整的、具有意义的词，而只是词的一部分，如“revival”词干提取的结果为“reviv”，“ailiner”词干提取的结果为“airlin”。而经词形还原处理后获得的结果是具有一定意义的、完整的词，一般为词典中的有效词。
在应用领域上，同样各有侧重。虽然二者均被应用于信息检索和文本处理中，但侧重不同。词干提取更多被应用于信息检索领域，如Solr、Lucene等，用于扩展检索，粒度较粗。词形还原更主要被应用于文本挖掘、自然语言处理，用于更细粒度、更为准确的文本分析和表达

## BOW(Bag of words)词袋模型
BOW原理十分简单，就是把一句话转换成计算机能看懂的向量形式，比如：

* I  love bag of words .
* I  have a bag of apples .

可以构建包含所有单词的词典向量vector =[I ,love ,a, bag, of , words, apples,have]

第一句话就可以用一个向量v1=[1,1,0,1,1,1,0,0]

第二句话的向量表示v2=[1,0,1,1,1,0,1,1]

向量的每一个维度表示词典向量中对应的单词在句子中出现的次数。那么bag of words 缺点是什么呢？

首先需要一个庞大的词典向量，这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（The Curse of Dimensionality）。其次，因为每个单词向量点乘的结果都是1,也就是余弦夹角等于90度，即每个单独的词汇之间完全没有关系。但是我们希望两个单词间的余弦值可以表示两个词汇之间的相似度。第三，bag of words 表达的语句完全跟单词顺序无关，而实际情况中词续对于句子十分重要。

接下来的Word2vec完美的解决了上面3个问题。

## Word2vec
这里只介绍skip-gram 模型,首先介绍两种不同的信息检索理论，它们都假设具有相似词向量分布的文本具有相似的语义信息，一种认为词频反映了语义信息，比如上文提到的Bag of words， 第二种认为上下文环境相似的词具有相似的语义信息。skip-gram 即基于第二种理论。

还是给定一个句子：I  have a bag of apples .

然后预设一个滑窗，大小可以自己指定，比如3， 用这个大小为3的滑窗在句子上面滑动，每个滑窗有一个中心词，两个上下文信息词汇。

假定词表中的每一个word都对应着一个连续的特征向量；

假定一个连续平滑的概率模型，输入一个中心词向量，可以输出所有次表中的词对应的概率；

而学习到的词向量要满足最大化上下文信息的条件概率。

## 中文自然语言处理的完整机器处理流程
https://www.jianshu.com/p/b87e01374a65