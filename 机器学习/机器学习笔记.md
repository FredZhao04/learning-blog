### 连续值特征做离散化的好处
在特征工程中，我们常常需要对连续型特征进行离散化处理，那么这么做有什么好处呢？

下面做简单的总结：

1.离散特征的增加和减少都很容易，易于模型的快速迭代；

2.系数矩阵内机乘法运算速度更快，计算结果方便存储，易于扩展；

3.离散化后的特征对异常数据有很强的鲁棒性。比如一个特征是年龄=300岁，会给模型造成很大的干扰；

4.单变量离散化N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；

5.离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；

6.特征离散化后，模型会更稳定，比如：用用户年龄离散化后，20-30作为一个区间，不会因为一个用户年龄增长1岁变成完全不同的人，当然处于区间相邻的样本会刚好相反，所以划分是门学问

7.特征离散化以后，起到了简化逻辑回归模型的的作用，降低模型过拟合的风险。

### 决策树和随机森林是否需要对连续数值进行离散化？
C4.5决策树算法中采用二分法（bi-partition）对连续属性进行离散化处理。

### 对非数值对象的处理
计算机对于数据处理形式只能是数据，因此现在假设数据是非数值的形式，具体形式可能是不同的类别，这时我们需要将类别转化为独热编码的形式才能对数据进行进一步处理。

思路1: 先将类别数据转换为整数类别，然后将整数类别转化为独热编码形式。需要用到Scikit-learn中LabelEncoder包与OneHotEncoder包。

思路2（一步到位） 利用Scikit-learn中的LabelBinarizer包。
### 决策树是否需要归一化？
决策树是概率模型。
概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。像svm、线性回归之类的最优化问题就需要归一化。决策树属于前者。
### 归一化
保留所有的特征，但是减小参数的大小。
归一化能够加快梯度下降的步伐，也就是获得最优解的速度。
归一化能够提高模型的精度。
### 哪些算法需要归一化？
|需要|不需要|
|----|----|
|LR（线性回归、逻辑回归）|决策树|
|SVM|随机森林|
|KNN|朴素贝叶斯|
|KMeans|XGBoost|
|高斯过程|lightGBM|
|AdaBoost|
|神经网络|
|LSTM|
|GBDT|
## 随机森林参数
### n_estimators and max_features
The main parameters to adjust when using these methods is n_estimators and max_features. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. **Empirical good default values are max_features=None (always considering all features instead of a random subset) for regression problems, and max_features="sqrt" (using a random subset of size sqrt(n_features)) for classification tasks (where n_features is the number of features in the data).** Good results are often achieved when setting max_depth=None in combination with min_samples_split=2 (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) while the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting oob_score=True.